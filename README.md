````markdown
# ğŸ“„ TinyLLaMA File Explainer App

A full-stack AI-powered application that explains files (code, configs, and text) using Ollama language models. Built with **FastAPI** for the backend and a lightweight **HTML/JS frontend UI**.

---

## ğŸ” Features

- âœ… **Chunked Processing** for large files
- ğŸ” **Model Fallback** to alternative LLMs if the primary fails
- ğŸ§  **CPU/GPU Support**
- ğŸ–¥ï¸ **System Monitoring**: memory/disk/Ollama status
- ğŸ”§ **Fix Models** on the fly via `/fix-model`
- ğŸ’» **Frontend Web Interface** included

---

## ğŸ“¸ Screenshot

> Save your screenshot in a folder like `screenshots/` and reference the image path.

### ğŸ§  Frontend UI

![Frontend UI](screenshots/frontend-ui.png)

---

## ğŸ§° Requirements

- Python 3.9+
- [Ollama](https://ollama.ai) installed and running
- Installed models: `tinyllama`, `llama2`, `gemma:2b`, `phi` (or edit fallback list)

---

## ğŸš€ Installation

### Backend Setup

```bash
pip install fastapi uvicorn httpx anyio

ollama serve

ollama pull tinyllama
ollama pull llama2
ollama pull gemma:2b
ollama pull phi
```
````

### Frontend Setup

```bash
cd frontend/
python -m http.server 8080
```

---

## â–¶ï¸ Running the App

### Backend

```bash
uvicorn backend:app --reload
```

- Opens: [http://localhost:8000](http://localhost:8000)

### Frontend

```bash
cd frontend/
python -m http.server 8080
```

- Opens: [http://localhost:8080](http://localhost:8080)

---

## ğŸ“¡ API Endpoints

- `POST /explain` â€” File explanation
- `GET /health` â€” Health check
- `GET /ollama-status` â€” Ollama status
- `POST /fix-model` â€” Re-pull broken model

---

## âš™ï¸ Configuration (`backend.py`)

| Setting              | Default             | Description               |
| -------------------- | ------------------- | ------------------------- |
| `OLLAMA_MODEL`       | `"tinyllama"`       | Primary model             |
| `FALLBACK_MODELS`    | `["llama2", "phi"]` | Fallback LLMs             |
| `MAX_CONTENT_LENGTH` | `5000`              | Max chars before chunking |
| `CPU_ONLY`           | `True`              | Force CPU only mode       |

---

## ğŸ§ª Sample Response

```json
{
  "explanation": "Section 1 Explanation:\nThis Python script prints 'Hello World'...",
  "status": "success"
}
```

---

## ğŸ› ï¸ Troubleshooting

- âŒ `"Failed to connect to Ollama"` â€” Run `ollama serve`
- âš ï¸ `"Empty explanation"` â€” Lower `MAX_CONTENT_LENGTH`
- ğŸ”’ `"Permission denied"` â€” Use `CPU_ONLY=True`
- ğŸ§± Model broken? Use `/fix-model`

## ğŸ§‘â€ğŸ’» Contributing

1. Fork this repo
2. Create a new branch
3. Make your changes
4. Open a Pull Request

---

## ğŸ“„ License

MIT License â€” See [LICENSE](LICENSE)
